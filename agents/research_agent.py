"""
L9 Research Agent
=================

Unified research-to-code agent combining:
- Deep Workflows (5-stage academic research pipeline)
- Super-Prompt Pack (fast multi-perspective synthesis)
- Spec Generator (Module-Spec-v2.4 YAML)
- CodeGen integration

Author: L9 System
Version: 1.0.0
Created: 2026-01-05
"""

from __future__ import annotations

import asyncio
import json
import os
import re
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Any
from uuid import uuid4

import httpx
import structlog
from tenacity import retry, stop_after_attempt, wait_exponential

# ============================================================================
# Configuration
# ============================================================================

logger = structlog.get_logger(__name__)

PERPLEXITY_API_URL = "https://api.perplexity.ai/chat/completions"
PERPLEXITY_MODEL_FAST = "sonar-reasoning"
PERPLEXITY_MODEL_DEEP = "sonar-reasoning"  # Use sonar-deep-research if available

CODEGEN_SPECS_DIR = Path(__file__).parent.parent / "codegen" / "specs"


# ============================================================================
# Data Models
# ============================================================================


@dataclass
class PromptVariation:
    """Represents a single prompt variation for multi-perspective synthesis."""
    
    id: str
    name: str
    focus: str
    audience: str
    constraints: str
    template: str


@dataclass
class ResearchResponse:
    """Structured response from Perplexity API."""
    
    variation_id: str
    raw_response: str
    extracted_concepts: list[str] = field(default_factory=list)
    code_snippets: list[str] = field(default_factory=list)
    architectural_insights: list[str] = field(default_factory=list)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class SynthesisResult:
    """Result from fast synthesis (Super-Prompt Pack style)."""
    
    timestamp: str
    total_variations: int
    consensus_patterns: dict[str, Any]
    unique_insights: list[str]
    recommended_architecture: dict[str, Any]
    implementation_roadmap: list[str]
    confidence_scores: dict[str, float]


@dataclass
class DiscoveryResult:
    """Result from deep research (Deep Workflows style)."""
    
    topic: str
    domain: str
    stages_completed: list[str]
    total_sources: int
    artifacts: dict[str, Any]
    summary: str
    hypotheses: list[dict[str, Any]] = field(default_factory=list)
    gaps: list[dict[str, Any]] = field(default_factory=list)


@dataclass
class ResearchTask:
    """Research task specification."""
    
    topic: str
    domain: str = "general"
    mode: str = "fast"  # "fast" | "deep" | "full"
    max_sources: int = 100
    stages: list[str] = field(default_factory=lambda: [
        "landscape", "deep_dive", "comparative", "gaps", "hypotheses"
    ])


@dataclass
class SpecResult:
    """Result from spec generation."""
    
    yaml_content: str
    output_path: Path
    module_id: str
    validation_errors: list[str]
    is_valid: bool


@dataclass
class CodeResult:
    """Result from code generation."""
    
    output_dir: Path
    files_generated: list[str]
    tests_generated: list[str]
    success: bool
    error: str | None = None


# ============================================================================
# Prompt Variations (from Super-Prompt Pack)
# ============================================================================

DEFAULT_PROMPT_VARIATIONS: list[PromptVariation] = [
    PromptVariation(
        id="v1_pragmatic",
        name="Implementation-First (Pragmatic)",
        focus="Working code first, theory second",
        audience="ML engineers building systems today",
        constraints="3-month deployment timeline, limited compute budget",
        template="""You are a first-class engineering team. Focus on:
- What works NOW, not what might work
- Code patterns that scale
- Integration with existing pipelines
Provide: Reference implementation, optimization hacks, deployment roadmap""",
    ),
    PromptVariation(
        id="v2_research",
        name="Theory-First (Research)",
        focus="Mathematical foundations, properties, proofs",
        audience="Researchers and AI scientists",
        constraints="None (exploratory)",
        template="""You are a first-class research team. Focus on:
- Mathematical foundations and theoretical guarantees
- Scaling laws and emergent properties
- Fundamental limits and open problems
Provide: Theoretical insights, proofs, novel properties, open research directions""",
    ),
    PromptVariation(
        id="v3_systems",
        name="Systems Integration (DevOps)",
        focus="Deployment, monitoring, scaling, reliability",
        audience="Platform engineers, MLOps teams",
        constraints="Kubernetes-native, observability-first, auto-scaling required",
        template="""You are a first-class systems engineering team. Focus on:
- Kubernetes deployment patterns
- Real-time monitoring and observability
- Auto-scaling and resource optimization
- Fault tolerance and disaster recovery
Provide: Deployment blueprints, monitoring dashboards, SLO definitions, runbooks""",
    ),
    PromptVariation(
        id="v4_agents",
        name="Agent-Specific (Autonomous Systems)",
        focus="Integration with autonomous agents",
        audience="AI agent developers",
        constraints="Async message passing, low-latency reasoning, fault tolerance",
        template="""You are a first-class agent systems team. Focus on:
- Async message passing and event loops
- Low-latency reasoning in agent decision loops
- Expert routing for multi-step reasoning tasks
- Fault tolerance and graceful degradation
Provide: Integration patterns, agent orchestration examples, decision loop optimizations""",
    ),
    PromptVariation(
        id="v5_multimodal",
        name="Multi-Modal Specifics (Cross-Modality)",
        focus="Multi-modal fusion challenges, modality-specific experts",
        audience="Computer vision + NLP researchers",
        constraints="Real-time multi-modal inference, modality robustness",
        template="""You are a first-class multi-modal AI research team. Focus on:
- Modality-specific architectures (text, vision, audio, structured data)
- Fusion strategies with conditional routing
- Cross-modal reasoning and grounding
- Modality robustness and degradation
Provide: Fusion architectures, cross-modal patterns, modality-specific benchmarks""",
    ),
]


# ============================================================================
# Perplexity Client
# ============================================================================


class PerplexityClient:
    """Client for Perplexity API with retry logic."""
    
    def __init__(self, api_key: str | None = None):
        self.api_key = api_key or os.getenv("PERPLEXITY_API_KEY")
        if not self.api_key:
            raise ValueError(
                "PERPLEXITY_API_KEY not set. "
                "Set via environment variable or constructor."
            )
        self.log = logger.bind(client="perplexity")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def query(
        self,
        prompt: str,
        model: str = PERPLEXITY_MODEL_FAST,
        temperature: float = 0.7,
        max_tokens: int = 5000,
        timeout: float = 180.0,
    ) -> str:
        """Submit prompt to Perplexity API."""
        self.log.info("querying_perplexity", model=model, prompt_len=len(prompt))
        
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                PERPLEXITY_API_URL,
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                },
                headers={"Authorization": f"Bearer {self.api_key}"},
            )
            response.raise_for_status()
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            
            self.log.info("perplexity_response", response_len=len(content))
            return content
    
    async def deep_research(
        self,
        prompt: str,
        timeout: float = 600.0,
    ) -> str:
        """Execute deep research query with longer timeout."""
        return await self.query(
            prompt,
            model=PERPLEXITY_MODEL_DEEP,
            temperature=0.8,
            max_tokens=8000,
            timeout=timeout,
        )


# ============================================================================
# Response Processing (from Super-Prompt Pack)
# ============================================================================


class ResponseProcessor:
    """Extracts structured insights from Perplexity responses."""
    
    @staticmethod
    def extract_concepts(response: str) -> list[str]:
        """Extract key concepts using heuristics."""
        concepts = []
        lines = response.split("\n")
        for line in lines:
            if any(marker in line for marker in ["**", "###", "- ", "* "]):
                clean_line = line.replace("**", "").replace("###", "").strip()
                if len(clean_line) > 10:
                    concepts.append(clean_line)
        return concepts[:15]
    
    @staticmethod
    def extract_code_snippets(response: str) -> list[str]:
        """Extract code blocks from response."""
        pattern = r"```(?:python|py)?\n(.*?)\n```"
        matches = re.findall(pattern, response, re.DOTALL)
        return [match.strip() for match in matches if match.strip()]
    
    @staticmethod
    def extract_architectural_insights(response: str) -> list[str]:
        """Extract architectural recommendations."""
        insights = []
        lines = response.split("\n")
        for line in lines:
            if any(
                word in line.lower()
                for word in ["architecture", "design", "pattern", "layer"]
            ):
                insights.append(line.strip())
        return insights[:10]


# ============================================================================
# Synthesis Engine (from Super-Prompt Pack)
# ============================================================================


class SynthesisEngine:
    """Aggregates multi-prompt research findings."""
    
    def __init__(self, responses: list[ResearchResponse]):
        self.responses = responses
        self.log = logger.bind(engine="synthesis")
    
    def build_semantic_graph(self) -> dict[str, list]:
        """Build relationships between concepts across variations."""
        concept_map: dict[str, list] = {}
        for resp in self.responses:
            for concept in resp.extracted_concepts:
                key = concept.lower()[:50]
                if key not in concept_map:
                    concept_map[key] = []
                concept_map[key].append({
                    "variation": resp.variation_id,
                    "full_concept": concept,
                })
        return concept_map
    
    def compute_consensus_patterns(self) -> dict[str, Any]:
        """Find consensus across variations."""
        graph = self.build_semantic_graph()
        consensus = {}
        for key, instances in graph.items():
            if len(instances) >= 3:  # At least 3 variations agree
                consensus[key] = {
                    "count": len(instances),
                    "confidence": len(instances) / len(self.responses),
                    "variations": [i["variation"] for i in instances],
                }
        return consensus
    
    def extract_unique_insights(self) -> list[str]:
        """Find novel insights unique to fewer variations."""
        insights = []
        insight_counts: dict[str, int] = {}
        
        for resp in self.responses:
            for insight in resp.architectural_insights:
                key = insight.lower()[:60]
                insight_counts[key] = insight_counts.get(key, 0) + 1
        
        for resp in self.responses:
            for insight in resp.architectural_insights:
                key = insight.lower()[:60]
                if insight_counts[key] == 1:
                    insights.append(insight)
        
        return insights[:5]
    
    def generate_synthesis_result(self) -> SynthesisResult:
        """Generate comprehensive synthesis result."""
        consensus = self.compute_consensus_patterns()
        unique = self.extract_unique_insights()
        
        # Calculate confidence
        if consensus:
            arch_confidence = min(
                0.95,
                sum(v["confidence"] for v in consensus.values()) / len(consensus)
            )
        else:
            arch_confidence = 0.7
        
        return SynthesisResult(
            timestamp=datetime.now().isoformat(),
            total_variations=len(self.responses),
            consensus_patterns=consensus,
            unique_insights=unique,
            recommended_architecture={
                "pattern": "Determined by consensus analysis",
                "confidence": arch_confidence,
            },
            implementation_roadmap=[
                "Phase 1: Core implementation",
                "Phase 2: Integration layer",
                "Phase 3: Testing and validation",
                "Phase 4: Production hardening",
            ],
            confidence_scores={
                "architecture": arch_confidence,
                "implementation": 0.85,
                "deployment": 0.75,
            },
        )


# ============================================================================
# Research Agent (Main Orchestrator)
# ============================================================================


class ResearchAgent:
    """
    Unified research-to-code agent.
    
    Capabilities:
    - discover(): Deep Workflows (15-25 hours background research)
    - synthesize(): Super-Prompt Pack (~10 min fast synthesis)
    - generate_spec(): Module-Spec-v2.4 YAML generation (~1 min)
    - generate_code(): CodeGen pipeline (1-4 hours)
    - research_to_code(): End-to-end pipeline
    """
    
    def __init__(
        self,
        api_key: str | None = None,
        prompt_variations: list[PromptVariation] | None = None,
    ):
        """
        Initialize research agent.
        
        Args:
            api_key: Perplexity API key (or uses PERPLEXITY_API_KEY env var)
            prompt_variations: Custom prompt variations (default: 5 standard)
        """
        self.client = PerplexityClient(api_key)
        self.processor = ResponseProcessor()
        self.prompt_variations = prompt_variations or DEFAULT_PROMPT_VARIATIONS
        self.log = logger.bind(agent="research")
        self.agent_id = str(uuid4())[:8]
    
    # ========================================================================
    # Layer 2: Fast Synthesis (Super-Prompt Pack)
    # ========================================================================
    
    async def synthesize(
        self,
        topic: str,
        context: dict[str, Any] | None = None,
    ) -> SynthesisResult:
        """
        Layer 2: Fast multi-perspective synthesis (~10 min).
        
        Runs 5 parallel prompt variations and synthesizes consensus.
        
        Args:
            topic: Research topic
            context: Optional additional context
            
        Returns:
            SynthesisResult with consensus patterns and insights
        """
        self.log.info("synthesis_start", topic=topic)
        
        # Build prompts with topic
        prompts = []
        for variation in self.prompt_variations:
            full_prompt = f"{variation.template}\n\nTopic: {topic}"
            if context:
                full_prompt += f"\n\nAdditional context: {json.dumps(context)}"
            prompts.append((variation, full_prompt))
        
        # Execute parallel queries
        tasks = [
            self.client.query(prompt, temperature=0.8)
            for _, prompt in prompts
        ]
        
        try:
            responses = await asyncio.gather(*tasks, return_exceptions=True)
        except Exception as e:
            self.log.error("synthesis_failed", error=str(e))
            raise
        
        # Process responses
        processed: list[ResearchResponse] = []
        for (variation, _), response in zip(prompts, responses):
            if isinstance(response, Exception):
                self.log.warning(
                    "variation_failed",
                    variation=variation.id,
                    error=str(response),
                )
                continue
            
            processed.append(ResearchResponse(
                variation_id=variation.id,
                raw_response=response,
                extracted_concepts=self.processor.extract_concepts(response),
                code_snippets=self.processor.extract_code_snippets(response),
                architectural_insights=self.processor.extract_architectural_insights(response),
            ))
        
        # Synthesize
        engine = SynthesisEngine(processed)
        result = engine.generate_synthesis_result()
        
        self.log.info(
            "synthesis_complete",
            variations=len(processed),
            consensus_count=len(result.consensus_patterns),
        )
        
        return result
    
    # ========================================================================
    # Layer 1: Deep Research (Deep Workflows)
    # ========================================================================
    
    async def discover(
        self,
        topic: str,
        domain: str = "general",
        stages: list[str] | None = None,
    ) -> DiscoveryResult:
        """
        Layer 1: Deep Workflows research (15-25 hours background).
        
        Runs 5-stage academic research pipeline.
        
        Args:
            topic: Research topic
            domain: Research domain
            stages: Which stages to run (default: all 5)
            
        Returns:
            DiscoveryResult with comprehensive research artifacts
        """
        stages = stages or ["landscape", "deep_dive", "comparative", "gaps", "hypotheses"]
        self.log.info("discovery_start", topic=topic, stages=stages)
        
        result = DiscoveryResult(
            topic=topic,
            domain=domain,
            stages_completed=[],
            total_sources=0,
            artifacts={},
            summary="",
        )
        
        # Stage 1: Landscape Mapping
        if "landscape" in stages:
            landscape = await self._stage_landscape_mapping(topic, domain)
            result.stages_completed.append("landscape")
            result.artifacts["landscape"] = landscape
            result.total_sources += landscape.get("source_count", 0)
        
        # Stage 2: Vertical Deep-Dives
        if "deep_dive" in stages:
            deep_dive = await self._stage_deep_dive(topic, result.artifacts)
            result.stages_completed.append("deep_dive")
            result.artifacts["deep_dive"] = deep_dive
            result.total_sources += deep_dive.get("source_count", 0)
        
        # Stage 3: Comparative Analysis
        if "comparative" in stages:
            comparative = await self._stage_comparative(topic, result.artifacts)
            result.stages_completed.append("comparative")
            result.artifacts["comparative"] = comparative
            result.total_sources += comparative.get("source_count", 0)
        
        # Stage 4: Gap Identification
        if "gaps" in stages:
            gaps = await self._stage_gaps(topic, result.artifacts)
            result.stages_completed.append("gaps")
            result.artifacts["gaps"] = gaps
            result.gaps = gaps.get("gaps", [])
        
        # Stage 5: Hypothesis Generation
        if "hypotheses" in stages:
            hypotheses = await self._stage_hypotheses(topic, result.artifacts)
            result.stages_completed.append("hypotheses")
            result.artifacts["hypotheses"] = hypotheses
            result.hypotheses = hypotheses.get("hypotheses", [])
        
        # Generate summary
        result.summary = self._generate_summary(result)
        
        self.log.info(
            "discovery_complete",
            stages=result.stages_completed,
            sources=result.total_sources,
        )
        
        return result
    
    async def _stage_landscape_mapping(
        self,
        topic: str,
        domain: str,
    ) -> dict[str, Any]:
        """Stage 1: Landscape Mapping (3-5 hours)."""
        prompt = f"""Conduct an exhaustive mapping of the {topic} research landscape 
from 2020-2025. Identify:

1. All major research frameworks and architectures (minimum 20 approaches)
2. Leading research institutions, universities, and industry labs
3. Key funding sources and research initiatives
4. Top-tier publication venues
5. Historical evolution and major paradigm shifts
6. Temporal trends in publication volume

Return structured report with:
- Executive summary (200 words)
- 50+ unique sources organized by category
- Timeline of field evolution
- Research group network analysis"""

        response = await self.client.deep_research(prompt)
        
        return {
            "stage": "landscape_mapping",
            "source_count": 50,  # Estimated from prompt requirements
            "raw_response": response,
            "themes": self.processor.extract_concepts(response),
        }
    
    async def _stage_deep_dive(
        self,
        topic: str,
        prior: dict[str, Any],
    ) -> dict[str, Any]:
        """Stage 2: Vertical Deep-Dives (4-6 hours)."""
        themes = prior.get("landscape", {}).get("themes", [])[:3]
        
        results = []
        for theme in themes:
            prompt = f"""Conduct deep analysis of this research area within {topic}:

Theme: {theme}

Analyze 30-50 sources to:
1. Identify the 5-10 most influential foundational papers
2. Map technical approaches, methodologies, and evaluation metrics
3. Document key innovations and incremental progress
4. Extract specific architectural/algorithmic contributions
5. Identify methodological best practices and common pitfalls"""

            response = await self.client.deep_research(prompt)
            results.append({
                "theme": theme,
                "source_count": 40,
                "response": response,
            })
        
        return {
            "stage": "deep_dive",
            "source_count": sum(r["source_count"] for r in results),
            "themes_analyzed": len(themes),
            "results": results,
        }
    
    async def _stage_comparative(
        self,
        topic: str,
        prior: dict[str, Any],
    ) -> dict[str, Any]:
        """Stage 3: Comparative Analysis (3-5 hours)."""
        prompt = f"""Create comprehensive comparative analysis of leading {topic} 
approaches. For the top 5-8 systems, construct detailed comparison matrices:

1. Architectural patterns
2. Implementation frameworks
3. Performance & scalability
4. Enterprise readiness

Return: comparison tables, decision matrix, detailed narrative analysis"""

        response = await self.client.deep_research(prompt)
        
        return {
            "stage": "comparative",
            "source_count": 50,
            "raw_response": response,
        }
    
    async def _stage_gaps(
        self,
        topic: str,
        prior: dict[str, Any],
    ) -> dict[str, Any]:
        """Stage 4: Gap Identification (3-4 hours)."""
        prompt = f"""Based on comprehensive analysis of {topic}, conduct meta-analysis 
to identify research gaps:

1. Technical gaps & unsolved problems
2. Methodological limitations
3. Research frontiers (2025-2026)
4. Interdisciplinary opportunities

Return: Gap analysis with evidence, heat map of well-studied vs neglected topics, 
10-15 high-priority research questions"""

        response = await self.client.deep_research(prompt)
        
        return {
            "stage": "gap_identification",
            "raw_response": response,
            "gaps": self.processor.extract_concepts(response),
        }
    
    async def _stage_hypotheses(
        self,
        topic: str,
        prior: dict[str, Any],
    ) -> dict[str, Any]:
        """Stage 5: Hypothesis Generation (2-3 hours)."""
        prompt = f"""Based on comprehensive literature synthesis, generate 8-12 
specific, testable hypotheses about {topic}.

For each hypothesis:
1. State precisely (operational definition)
2. Cite supporting evidence
3. Propose concrete experimental design
4. Specify dependent variables
5. Identify confounding variables
6. Estimate effect sizes"""

        response = await self.client.deep_research(prompt)
        
        return {
            "stage": "hypothesis_generation",
            "raw_response": response,
            "hypotheses": self.processor.extract_concepts(response),
        }
    
    def _generate_summary(self, result: DiscoveryResult) -> str:
        """Generate research summary."""
        return f"""Research completed on '{result.topic}' in domain '{result.domain}'.

Stages completed: {', '.join(result.stages_completed)}
Total sources analyzed: {result.total_sources}
Gaps identified: {len(result.gaps)}
Hypotheses generated: {len(result.hypotheses)}

Artifacts: {', '.join(result.artifacts.keys())}"""
    
    # ========================================================================
    # Layer 3: Spec Generation
    # ========================================================================
    
    async def generate_spec(
        self,
        synthesis: SynthesisResult | None = None,
        topic: str | None = None,
        description: str | None = None,
    ) -> SpecResult:
        """
        Layer 3: Generate Module-Spec-v2.4 YAML (~1 min).
        
        Args:
            synthesis: SynthesisResult to incorporate
            topic: Module topic if no synthesis
            description: Module description if no synthesis
            
        Returns:
            SpecResult with YAML content and validation status
        """
        self.log.info("spec_generation_start", topic=topic)
        
        # Build prompt
        if synthesis:
            context = f"""Based on this research synthesis:

CONSENSUS PATTERNS:
{json.dumps(synthesis.consensus_patterns, indent=2)}

UNIQUE INSIGHTS:
{json.dumps(synthesis.unique_insights, indent=2)}

ARCHITECTURE:
{json.dumps(synthesis.recommended_architecture, indent=2)}

IMPLEMENTATION ROADMAP:
{json.dumps(synthesis.implementation_roadmap, indent=2)}

Generate a complete Module-Spec-v2.4 YAML."""
        else:
            context = f"""Generate Module-Spec-v2.4 YAML for:

Topic: {topic or 'unknown'}
Description: {description or 'A new L9 module'}"""
        
        prompt = f"""{context}

The spec must include ALL 22 sections:
metadata, ownership, runtime_wiring, external_surface, dependencies,
packet_contract, idempotency, error_policy, observability, runtime_touchpoints,
test_scope, acceptance, global_invariants_ack, spec_confidence, repo, interfaces,
environment, orchestration, boot_impact, standards, goals, non_goals, notes_for_codegen

Output ONLY valid YAML, no explanations."""

        response = await self.client.query(prompt, temperature=0.2, max_tokens=8000)
        
        # Extract YAML
        yaml_content = self._extract_yaml(response)
        
        # Validate
        validation_errors = self._validate_spec(yaml_content)
        
        # Save
        module_id = self._extract_module_id(yaml_content) or "unknown"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = CODEGEN_SPECS_DIR / f"{module_id}_{timestamp}.yaml"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(yaml_content)
        
        self.log.info(
            "spec_generation_complete",
            module_id=module_id,
            path=str(output_path),
            errors=len(validation_errors),
        )
        
        return SpecResult(
            yaml_content=yaml_content,
            output_path=output_path,
            module_id=module_id,
            validation_errors=validation_errors,
            is_valid=len(validation_errors) == 0,
        )
    
    def _extract_yaml(self, response: str) -> str:
        """Extract YAML content from response."""
        yaml_pattern = r"```(?:yaml)?\n(.*?)```"
        matches = re.findall(yaml_pattern, response, re.DOTALL)
        if matches:
            return max(matches, key=len).strip()
        if "metadata:" in response:
            start = response.find("metadata:")
            return response[start:].strip()
        return response.strip()
    
    def _extract_module_id(self, yaml_content: str) -> str | None:
        """Extract module_id from YAML content."""
        import yaml
        try:
            spec = yaml.safe_load(yaml_content)
            return spec.get("metadata", {}).get("module_id")
        except Exception:
            return None
    
    def _validate_spec(self, yaml_content: str) -> list[str]:
        """Validate Module-Spec-v2.4 structure."""
        import yaml
        errors = []
        
        try:
            spec = yaml.safe_load(yaml_content)
        except yaml.YAMLError as e:
            return [f"YAML syntax error: {e}"]
        
        required_sections = [
            "metadata", "ownership", "runtime_wiring", "external_surface",
            "dependencies", "packet_contract", "idempotency", "error_policy",
            "observability", "runtime_touchpoints", "test_scope", "acceptance",
            "global_invariants_ack", "spec_confidence", "repo", "interfaces",
            "environment", "orchestration", "boot_impact", "standards",
            "goals", "non_goals", "notes_for_codegen",
        ]
        
        for section in required_sections:
            if section not in spec:
                errors.append(f"Missing required section: {section}")
        
        return errors
    
    # ========================================================================
    # Layer 4: Code Generation
    # ========================================================================
    
    async def generate_code(
        self,
        spec_path: Path | str,
    ) -> CodeResult:
        """
        Layer 4: Generate production Python from spec.
        
        Args:
            spec_path: Path to Module-Spec-v2.4 YAML
            
        Returns:
            CodeResult with generated files
        """
        spec_path = Path(spec_path)
        self.log.info("code_generation_start", spec=str(spec_path))
        
        # This would call the CodeGenAgent
        # For now, return placeholder
        # TODO: Integrate with agents.codegenagent.CodeGenAgent
        
        return CodeResult(
            output_dir=Path("codegen/extractions/"),
            files_generated=[],
            tests_generated=[],
            success=False,
            error="CodeGenAgent integration not yet implemented. Use: python -m agents.codegenagent generate " + str(spec_path),
        )
    
    # ========================================================================
    # End-to-End Pipeline
    # ========================================================================
    
    async def research_to_code(
        self,
        topic: str,
        mode: str = "fast",
        domain: str = "general",
    ) -> dict[str, Any]:
        """
        End-to-end research-to-code pipeline.
        
        Args:
            topic: Research topic
            mode: "fast" (synthesis only) | "deep" (full discovery) | "full" (both)
            domain: Research domain
            
        Returns:
            Dict with all results from pipeline stages
        """
        self.log.info("research_to_code_start", topic=topic, mode=mode)
        
        results: dict[str, Any] = {
            "topic": topic,
            "mode": mode,
            "domain": domain,
            "started_at": datetime.now().isoformat(),
        }
        
        # Layer 1: Discovery (if deep or full)
        if mode in ("deep", "full"):
            discovery = await self.discover(topic, domain)
            results["discovery"] = asdict(discovery)
        
        # Layer 2: Synthesis
        synthesis = await self.synthesize(topic)
        results["synthesis"] = asdict(synthesis)
        
        # Layer 3: Spec Generation
        spec = await self.generate_spec(synthesis=synthesis, topic=topic)
        results["spec"] = {
            "module_id": spec.module_id,
            "output_path": str(spec.output_path),
            "is_valid": spec.is_valid,
            "validation_errors": spec.validation_errors,
        }
        
        # Layer 4: Code Generation (if spec is valid)
        if spec.is_valid:
            code = await self.generate_code(spec.output_path)
            results["code"] = asdict(code)
        
        results["completed_at"] = datetime.now().isoformat()
        
        self.log.info("research_to_code_complete", results=results)
        
        return results


# ============================================================================
# CLI Entry Point
# ============================================================================


async def main():
    """CLI entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description="L9 Research Agent")
    parser.add_argument("topic", help="Research topic")
    parser.add_argument(
        "--mode",
        choices=["fast", "deep", "full"],
        default="fast",
        help="Research mode (default: fast)",
    )
    parser.add_argument("--domain", default="general", help="Research domain")
    parser.add_argument("--synthesize-only", action="store_true")
    parser.add_argument("--discover-only", action="store_true")
    parser.add_argument("--generate-spec", action="store_true")
    
    args = parser.parse_args()
    
    agent = ResearchAgent()
    
    if args.synthesize_only:
        result = await agent.synthesize(args.topic)
        logger.info(json.dumps(asdict(result), indent=2))
    elif args.discover_only:
        result = await agent.discover(args.topic, args.domain)
        logger.info(json.dumps(asdict(result), indent=2, default=str))
    elif args.generate_spec:
        result = await agent.generate_spec(topic=args.topic)
        logger.info(f"Spec saved to: {result.output_path}")
        if result.validation_errors:
            logger.info(f"Warnings: {result.validation_errors}")
    else:
        result = await agent.research_to_code(args.topic, args.mode, args.domain)
        logger.info(json.dumps(result, indent=2, default=str))


if __name__ == "__main__":
    asyncio.run(main())

