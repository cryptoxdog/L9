# STRATEGY MEMORY: DEVELOPER REFERENCE CARD (Quick Lookup)\n\n---\n\n## ðŸš€ QUICK REFERENCE\n\n### **What is Strategy Memory?**\nA **persistent cache of past task executions** stored as strategy graphs in Neo4j. When a new task arrives, the system retrieves similar past strategies, adapts them to the current context, and executes themâ€”falling back to de novo planning if confidence is too low.\n\n### **Core Loop (5 Steps)**\n```\n1. Task Input â”€â”€â–¶ CEO Agent (classify) â”€â”€â–¶ Semantic Embedding\n2. Embedding â”€â”€â–¶ Strategy Matcher (Neo4j) â”€â”€â–¶ Top-5 Strategies (sorted by score)\n3. Match? â”€â”€â–¶ YES: RAFA Adapter (refine) â”€â”€â–¶ Adapted Strategy\n        â””â”€â”€â–¶ NO: De Novo Planning â”€â”€â–¶ Fresh Strategy\n4. Execute â”€â”€â–¶ Agent Executor â”€â”€â–¶ Outcome Metrics\n5. Update â”€â”€â–¶ Neo4j (score, tags, lineage) + Agent Q Learn\n```\n\n---\n\n## ðŸŽ¯ COMPONENTS AT A GLANCE\n\n| Component | Input | Process | Output | Tech Stack |\n|-----------|-------|---------|--------|------------|\n| **Semantic Layer** | Task description | Encode to 384-dim vector | task_embedding | pgvector |\n| **Strategy Matcher** | task_embedding + goal_sig | Hybrid scoring (3 signals) | Ranked List[Strategy] | Neo4j + GDS |\n| **RAFA Adapter** | Strategy + context | Prune + re-optimize | Adapted graph + confidence | Graph editing |\n| **CoPlanner** | Adapted strategy | Sequence tasks + resolve deps | Task network | Orchestration |\n| **Executor** | Task network | Execute primitives + monitor | Outcome dict | Agent loop |\n| **Updater** | Outcome | Score + tags + drift check | Neo4j transaction | PacketEnvelope |\n| **Agent Q** | Outcome â†’ Q-value | Learn task â†’ strategy mapping | Q-weights | DQN |\n\n---\n\n## ðŸ“ CRITICAL DATA STRUCTURES\n\n### **Strategy Node (Neo4j)**\n```python\nStrategy {\n    id: UUID,\n    context_embedding: Vector[384],      # Query with: vector_similarity()\n    graph_signature: String,              # Query with: graph_edit_distance()\n    performance_score: Float [0-1],       # Sort by: DESC\n    generality_score: Float [0-1],        # Track: % tasks adapted to\n    tags: [String],                       # Filter by: \"planning\", \"fast\", etc.\n    adaptation_history: [Dict],           # Debug: why did it fail?\n    failure_rate: Float [0-1],            # Trigger self-repair if > 0.2\n    age_days: Int                         # Archive if > 60 & low score\n}\n```\n\n### **Retrieval Query (Pseudocode)**\n```cypher\nMATCH (s:Strategy)\nWHERE \n    vector_similarity(s.context_embedding, $task_vec) > 0.5 OR\n    graph_edit_distance(s.graph_signature, $goal_sig) < 3 OR\n    any(tag in s.tags WHERE tag in $preferred_tags)\nWITH s, \n    0.4 * vector_sim + \n    0.4 * (1 / (1 + graph_dist)) + \n    0.2 * tag_match as score\nWHERE score > 0.6\nRETURN s ORDER BY s.performance_score DESC LIMIT 5\n```\n\n---\n\n## ðŸ”§ INTEGRATION POINTS\n\n### **Packet Types (Add to substrate_service.py)**\n```python\nStrategyRequest = PacketEnvelope[\n    metadata={\n        packet_type: \"strategy_query\",\n        task_embedding: Vector[384],\n        goal_description: String,\n        deadline_ms: Int\n    },\n    payload={\n        current_state: Dict,\n        task_context: Dict\n    }\n]\n\nStrategyResponse = PacketEnvelope[\n    metadata={packet_type: \"strategy_response\"},\n    payload={\n        matched_strategies: [Strategy],\n        confidence_scores: [Float],\n        coplanner_input: Dict\n    }\n]\n\nStrategyFeedback = PacketEnvelope[\n    metadata={packet_type: \"strategy_execution_outcome\"},\n    payload={\n        success: Bool,\n        execution_time_ms: Int,\n        resource_cost: Float,\n        failure_reason: Optional[String]\n    }\n]\n```\n\n### **Policy Engine Integration (Add to policy_engine.yml)**\n```yaml\npolicies:\n  strategy_memory:\n    - action: \"retrieve_from_strategy_store\"\n      effect: \"read_only\"\n      approval_required: false\n    \n    - action: \"execute_adapted_strategy\"\n      effect: \"side_effect\"\n      approval_required: \"if confidence < 0.6\"\n      constraints:\n        - max_adaptation_distance: 3\n        - min_confidence: 0.5\n        - timeout_ms: 5000\n```\n\n---\n\n## âš¡ LATENCY BUDGET (SLA)\n\n| Operation | Target | Actual (Baseline) | Notes |\n|-----------|--------|-------------------|-------|\n| Embedding generation | <30 ms | ? | Substrate layer |\n| Strategy retrieval (Neo4j) | <50 ms | ? | Hybrid scoring |\n| RAFA adaptation | <150 ms | ? | Pruning + optimization |\n| Execution + feedback | <1000 ms | ? | Task-dependent |\n| Neo4j update | <200 ms | ? | Transaction |\n| **Total (reuse path)** | **<400 ms** | â€” | vs. 500 ms de novo |\n\n---\n\n## ðŸ“Š METRICS TO TRACK\n\n### **Real-Time Dashboard (Every Task)**\n```\nRetrieval:\n  - matched_count: int           # 0-5 per query\n  - top_confidence: float        # 0.0-1.0\n  - retrieval_latency_ms: int    # <50 ms target\n  - false_positive_rate: float   # <5% target\n\nExecution:\n  - success: bool\n  - execution_time_ms: int       # vs. baseline ~500 ms\n  - resource_cost: float         # normalized\n  - failure_reason: str          # null if success\n\nStrategy Quality:\n  - performance_score: float     # [0-1], exponential smoothing\n  - generality_score: float      # [0-1], % tasks adapted to\n  - age_days: int                # < 30 target\n  - failure_rate: float          # rolling window, trigger repair if > 0.2\n```\n\n### **Weekly Audit**\n```\n-- Strategy library health\nSELECT \n  COUNT(*) as total_strategies,\n  COUNT(CASE WHEN performance_score > 0.7 THEN 1 END) as high_quality,\n  AVG(age_days) as avg_age,\n  SUM(usage_count) as total_executions\nFROM Strategy;\n\n-- Reuse effectiveness\nSELECT\n  SUM(CASE WHEN was_adapted THEN 1 END) / COUNT(*) as reuse_rate,\n  AVG(execution_time_ms) as avg_time_reused,\n  SUM(CASE WHEN success THEN 1 END) / COUNT(*) as success_rate\nFROM Execution\nWHERE timestamp > now() - interval '7 days';\n```\n\n---\n\n## ðŸ› DEBUGGING CHECKLIST\n\n### **\"Retrieval is slow (>100ms)\"**\n- [ ] Check Neo4j index on context_embedding (pgvector)\n- [ ] Verify vector_similarity query plan (explain)\n- [ ] Profile graph_edit_distance computation (GDS)\n- [ ] Check Neo4j cache hit rate\n\n### **\"Matched strategy fails during execution\"**\n- [ ] Was confidence > 0.6? (should be)\n- [ ] What was adaptation_distance? (< 3 expected)\n- [ ] Check RAFA logs for pruning decisions\n- [ ] Was precondition check skipped?\n\n### **\"Strategy library keeps growing (bloat)\"**\n- [ ] Run monthly prune: `DELETE Strategy WHERE performance_score < 0.3 AND age_days > 30`\n- [ ] Check for duplicate strategies: `GROUP BY graph_signature HAVING COUNT(*) > 1`\n- [ ] Archive old executions: `MATCH (e:Execution) WHERE e.timestamp < now() - 90d DELETE e`\n\n### **\"Agent Q not converging\"**\n- [ ] Are rewards properly scaled? (0-1 range)\n- [ ] Is exploration rate decaying? (epsilon-greedy)\n- [ ] Check for negative transfer: is Agent Q choosing bad strategies?\n- [ ] Verify task embedding stability (should be consistent for same task type)\n\n---\n\n## ðŸš¨ FAILURE MODES & FIXES\n\n| Symptom | Root Cause | Fix |\n|---------|-----------|-----|\n| **Retrieval matches but execution fails** | Overly permissive threshold or bad adaptation | Lower match threshold (0.6 â†’ 0.7); increase required adaptation distance |\n| **No matches found for any task** | Embeddings poorly calibrated or task distribution shifted | Retrain embedding model; check vector dimensions (should be 384) |\n| **Strategy performance degrades over time** | Feedback loop not updating scores, old context assumptions | Check exponential smoothing: `score = 0.7 * old + 0.3 * new` |\n| **Memory bloat, Neo4j queries slow** | Old strategies not pruned, execution logs accumulate | Run monthly cleanup: prune score < 0.3, archive old executions |\n| **False positives despite high confidence** | Embedding space not separating dissimilar tasks | Check perturbation test: noisy embeddings should still retrieve correct strategy |\n\n---\n\n## ðŸ’¾ DATABASE OPERATIONS\n\n### **Create Strategy (Post-Execution)**\n```cypher\nCREATE (s:Strategy {\n    id: apoc.create.uuid(),\n    name: $name,\n    context_embedding: $embedding,  // 384-dim vector\n    graph_signature: apoc.hashing.sha256($graph_json),\n    performance_score: 1.0,  // Start optimistic\n    generality_score: 0.0,   // Will increase with adaptations\n    creation_datetime: datetime(),\n    last_used: datetime(),\n    usage_count: 0,\n    tags: $tags,\n    adaptation_history: [],\n    failure_rate: 0.0,\n    age_days: 0\n})\nWITH s\nFOREACH (task in $tasks |\n    CREATE (t:Task {\n        id: apoc.create.uuid(),\n        strategy_id: s.id,\n        order: task.order,\n        type: task.type,\n        agent_target: task.agent_target,\n        name: task.name,\n        parameters: task.parameters,\n        depends_on: task.depends_on,\n        coordinates_with: task.coordinates_with\n    })\n    CREATE (s)-[:DECOMPOSES_INTO]->(t)\n)\nRETURN s;\n```\n\n### **Update Strategy Score (After Feedback)**\n```cypher\nMATCH (s:Strategy {id: $strategy_id})\nSET s.performance_score = 0.7 * s.performance_score + 0.3 * $outcome_score,\n    s.last_used = datetime(),\n    s.usage_count = s.usage_count + 1,\n    s.age_days = duration.between(s.creation_datetime, datetime()).days\nWITH s\nCREATE (s)-[:EXECUTED_AS]->(\n    :Execution {\n        id: apoc.create.uuid(),\n        success: $success,\n        execution_time_ms: $time_ms,\n        resource_cost: $cost,\n        failure_reason: $reason,\n        timestamp: datetime(),\n        was_adapted: $was_adapted,\n        adaptation_distance: $adapt_dist\n    }\n)\nRETURN s.performance_score;\n```\n\n---\n\n## ðŸ“ž COMMON QUESTIONS\n\n**Q: Can I skip the embedding step and just use graph edit distance?**  \nA: No. Graph alone misses semantic context (e.g., \"move object left\" vs. \"move object right\" have identical graph structure). Use hybrid: 40% embedding + 40% graph + 20% symbolic.\n\n**Q: What if adapted strategy fails? Do we try de novo?**  \nA: Yes. Confidence threshold prevents most failures. If execution fails anyway, log failure_reason, decay strategy score, and fall back to de novo for that task.\n\n**Q: How often do I need to retrain embeddings?**  \nA: Quarterly (every 13 weeks). If task distribution shifts significantly, retrain sooner. Watch for: retrieval precision <80% or false positive rate >5%.\n\n**Q: Can I use this with heterogeneous agents (RAFA + Agent Q + CoPlanner)?**  \nA: Yes. Each agent receives a task network from Strategy Matcher; agents coordinate via CoPlanner. Feedback loop updates single strategy record.\n\n**Q: What's the max strategy library size?**  \nA: Target 5-10k active strategies. Neo4j can handle 100k+, but pruning low-scoring keeps library lean and retrieval fast.\n\n---\n\n## ðŸ“š FILE GUIDE\n\n| File | When to Read | What's Inside |\n|------|--------------|---------------|\n| **strategy_memory_executive_summary.md** | Start here | Business case, hypothesis, research foundation |\n| **strategy_memory_research_map.md** | Detailed design | Full papers, code examples, integration mapping |\n| **strategy_memory_checklist.md** | Implementation | Phase-by-phase tasks, Neo4j schema, success metrics |\n| **strategy_memory_visual_guide.md** | Operations | Architecture diagrams, data structures, procedures |\n| This file (Reference Card) | Daily development | Quick lookup, debugging, metrics |\n\n---\n\n**Version:** 1.0.0 | **Date:** 2025-12-19 | **Status:** âœ… Ready\n\n**Need help?** â†’ See `strategy_memory_research_map.md` (Section: INTEGRATION ARCHITECTURE)\n